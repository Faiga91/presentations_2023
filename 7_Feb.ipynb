{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "784780e8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src='logo.png'/></center>\n",
    "\n",
    "# <center> InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "    \n",
    "<center> Authors: Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, Pieter Abbeel \n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<center>Journal Club | SIMULAMET | Faiga Alawad\n",
    "<br> 7 Feb 2023\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8393d45",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3057c392",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Generative models "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2ecfdc",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Generative models are machine learning models that are used to generate new data samples that are similar to a training dataset. The goal of a generative model is to learn the underlying distribution of the data and use this information to generate new samples that are representative of the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75f6ea1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Entangled representations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a081c09",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The need for disentangled representations arises from the problem of entangled representations in generative models. In entangled representations, the latent factors of the generative model are highly correlated and cannot be interpreted as separate, semantically meaningful attributes of the data. This makes it difficult to control the properties of the generated samples and to gain insight into the underlying data distribution.\n",
    "\n",
    "In entangled representations, it is difficult to control the properties of the generated samples and to gain insight into the underlying data distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d46059",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Disentangled representations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a664fff6",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Disentangled representations, on the other hand, aim to separate the latent factors of the generative model so that each factor corresponds to a specific attribute of the data. This makes it easier to control the properties of the generated samples and to gain a deeper understanding of the underlying data distribution. The goal of many recent generative models, such as InfoGAN, is to learn disentangled representations of the data that allow for fine-grained control over the properties of the generated samples.\n",
    "\n",
    "This enables fine-grained control over the properties of the generated samples and allows for a deeper understanding of the underlying data distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8da17c2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e93097",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src='GANs.png'/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58d33e4",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Generative Adversarial Networks (GANs) are a type of deep learning architecture that can be used to generate new data samples that are similar to a given training set. GANs consist of two main components: a generator network and a discriminator network.\n",
    "\n",
    "The generator network is responsible for generating new data samples, while the discriminator network is responsible for evaluating the quality of the generated samples. The generator and discriminator networks are trained together in an adversarial manner, where the generator is trying to produce samples that are indistinguishable from the training set, and the discriminator is trying to correctly identify whether each sample is real or fake.\n",
    "\n",
    "The adversarial training process involves alternating between updating the generator network to generate better samples, and updating the discriminator network to become better at differentiating between real and fake samples. The goal is to find a Nash equilibrium, where the generator is generating high-quality samples that are indistinguishable from the training set, and the discriminator is unable to further improve its performance.\n",
    "\n",
    "The generator network is usually trained using backpropagation, with the objective of minimizing a loss function that measures the difference between the generated samples and the real samples from the training set. The discriminator network is trained using supervised learning, with the objective of correctly classifying each sample as real or fake."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18778949",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## InfoGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfaadc07",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src='InfoGAN.png'/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a90c9c",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "InfoGAN is a variant of Generative Adversarial Networks (GANs) that aims to disentangle the factors of variation in the generated data. It does so by adding a code vector to the generator network, which represents interpretable, disentangled factors of variation.\n",
    "\n",
    "The architecture of InfoGAN consists of four main components: the noise vector, the code vector, the generator network, and the discriminator network.\n",
    "\n",
    "The noise vector is a random input vector that is used to generate diverse samples. This is similar to the noise vector in traditional GANs, but in InfoGAN, the noise vector has an additional role of providing randomness to the generator.\n",
    "\n",
    "The code vector is a set of interpretable, disentangled factors of variation. The code vector can be used to control the generated samples by adjusting the values of the disentangled factors.\n",
    "\n",
    "The generator network is responsible for transforming the noise vector and the code vector into generated samples. The generator network has two inputs: the noise vector and the code vector. The code vector is transformed into a set of intermediate variables that can be used to control the generated samples.\n",
    "\n",
    "The discriminator network is responsible for evaluating the quality of the generated samples. The discriminator network takes a sample as input and outputs a score that indicates how likely the sample is to be a real sample from the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8e86f2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97729616",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "1. Adversarial Loss\n",
    "\n",
    "$L_{adv} = -E_x[log(D(x))] - E_z[log(1 - D(G(z)))]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aedd6e0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "2. Mutual Information Loss\n",
    "\n",
    "$L_{info} = -E_z[I(c, G(z))]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24696ac0",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The mutual information loss is used to encourage the code vector to represent disentangled, interpretable factors of variation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c57763",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "3. Entropy Regulatization \n",
    "\n",
    "$L_{ent} = E_z[H(c)]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228fa39d",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The entropy regularization term is used to encourage the code vector to be as random as possible, in order to avoid overfitting to a specific set of disentangled factors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdad2eeb",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$L = L_{adv} + lambda_{info} \\times L_{info} + lambda_{ent} \\times L_{ent} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b384d1e",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "where lambda_info and lambda_ent are hyperparameters that control the importance of the mutual information loss and the entropy regularization term, respectively.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e74c7f3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49406731",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src='Results.png'/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67569c18",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src='Results_3d.png'/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9b19fc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b735bdb9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "InfoGAN code implementaiton (Github):\n",
    "https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/infogan/infogan.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373f8658",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4314b5ec",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Chen, Xi, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. “InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets.” arXiv, June 11, 2016. https://doi.org/10.48550/arXiv.1606.03657.\n",
    "\n",
    "\n",
    "<br> Credit to ChatGPT for generating the notes in this presentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cf620b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
